{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set environment\n",
    "import os\n",
    "import sys\n",
    " \n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Sparksession driver\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Classification of Student dataset\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----------+----------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|instr|class|nbrepeat|attendance|difficulty| Q1| Q2| Q3| Q4| Q5| Q6| Q7| Q8| Q9|Q10|Q11|Q12|Q13|Q14|Q15|Q16|Q17|Q18|Q19|Q20|Q21|Q22|Q23|Q24|Q25|Q26|Q27|Q28|\n",
      "+-----+-----+--------+----------+----------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|    1|    2|       1|         0|         4|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|\n",
      "|    1|    2|       1|         1|         3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|\n",
      "|    1|    2|       1|         2|         4|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|  5|\n",
      "|    1|    2|       1|         1|         3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|  3|\n",
      "|    1|    2|       1|         0|         1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "+-----+-----+--------+----------+----------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student = spark.read.csv('data/student_copy.csv',header=True,inferSchema=True)\n",
    "student.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of cells in column instr with null values: 0\n",
      "no. of cells in column class with null values: 0\n",
      "no. of cells in column nbrepeat with null values: 0\n",
      "no. of cells in column attendance with null values: 0\n",
      "no. of cells in column difficulty with null values: 0\n",
      "no. of cells in column Q1 with null values: 0\n",
      "no. of cells in column Q2 with null values: 0\n",
      "no. of cells in column Q3 with null values: 0\n",
      "no. of cells in column Q4 with null values: 0\n",
      "no. of cells in column Q5 with null values: 0\n",
      "no. of cells in column Q6 with null values: 0\n",
      "no. of cells in column Q7 with null values: 0\n",
      "no. of cells in column Q8 with null values: 0\n",
      "no. of cells in column Q9 with null values: 0\n",
      "no. of cells in column Q10 with null values: 0\n",
      "no. of cells in column Q11 with null values: 0\n",
      "no. of cells in column Q12 with null values: 0\n",
      "no. of cells in column Q13 with null values: 0\n",
      "no. of cells in column Q14 with null values: 0\n",
      "no. of cells in column Q15 with null values: 0\n",
      "no. of cells in column Q16 with null values: 0\n",
      "no. of cells in column Q17 with null values: 0\n",
      "no. of cells in column Q18 with null values: 0\n",
      "no. of cells in column Q19 with null values: 0\n",
      "no. of cells in column Q20 with null values: 0\n",
      "no. of cells in column Q21 with null values: 0\n",
      "no. of cells in column Q22 with null values: 0\n",
      "no. of cells in column Q23 with null values: 0\n",
      "no. of cells in column Q24 with null values: 0\n",
      "no. of cells in column Q25 with null values: 0\n",
      "no. of cells in column Q26 with null values: 0\n",
      "no. of cells in column Q27 with null values: 0\n",
      "no. of cells in column Q28 with null values: 0\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "for col in student.columns:\n",
    "    print(\"no. of cells in column\", col, \"with null values:\", student.filter(student[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COPYING THE COLUMNS EXCLUSING TARGET COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=student.drop('instr')\n",
    "dfs_colnames = dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([2.0, 1.0, 0.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0])),\n",
       " Row(features=DenseVector([2.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0])),\n",
       " Row(features=DenseVector([2.0, 1.0, 2.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "assembler = VectorAssembler(inputCols= dfs_colnames, \n",
    "                            outputCol=\"features\")\n",
    "feature_vec=assembler.transform(student)\n",
    "feature_vec.select(\"features\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|instr|count|\n",
      "+-----+-----+\n",
      "|    1|  775|\n",
      "|    3| 3601|\n",
      "|    2| 1444|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count of target classes\n",
    "feature_vec.groupBy('instr').count().show()\n",
    "#there is data imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGEDTHETYPE OF COLUMN INSTR TO DOUBLE ,so THAT I COULD CHECK METRIC AS CONFUSION METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "changedTypedf = feature_vec.withColumn(\"instr\", feature_vec[\"instr\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = changedTypedf.randomSplit([.75,.25],seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- instr: double (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      " |-- nbrepeat: integer (nullable = true)\n",
      " |-- attendance: integer (nullable = true)\n",
      " |-- difficulty: integer (nullable = true)\n",
      " |-- Q1: integer (nullable = true)\n",
      " |-- Q2: integer (nullable = true)\n",
      " |-- Q3: integer (nullable = true)\n",
      " |-- Q4: integer (nullable = true)\n",
      " |-- Q5: integer (nullable = true)\n",
      " |-- Q6: integer (nullable = true)\n",
      " |-- Q7: integer (nullable = true)\n",
      " |-- Q8: integer (nullable = true)\n",
      " |-- Q9: integer (nullable = true)\n",
      " |-- Q10: integer (nullable = true)\n",
      " |-- Q11: integer (nullable = true)\n",
      " |-- Q12: integer (nullable = true)\n",
      " |-- Q13: integer (nullable = true)\n",
      " |-- Q14: integer (nullable = true)\n",
      " |-- Q15: integer (nullable = true)\n",
      " |-- Q16: integer (nullable = true)\n",
      " |-- Q17: integer (nullable = true)\n",
      " |-- Q18: integer (nullable = true)\n",
      " |-- Q19: integer (nullable = true)\n",
      " |-- Q20: integer (nullable = true)\n",
      " |-- Q21: integer (nullable = true)\n",
      " |-- Q22: integer (nullable = true)\n",
      " |-- Q23: integer (nullable = true)\n",
      " |-- Q24: integer (nullable = true)\n",
      " |-- Q25: integer (nullable = true)\n",
      " |-- Q26: integer (nullable = true)\n",
      " |-- Q27: integer (nullable = true)\n",
      " |-- Q28: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"instr\", featuresCol=\"features\",  \n",
    "                        maxIter=100, regParam=0.0001, family=\"multinomial\",  \n",
    "                        elasticNetParam=0.0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(train_data)\n",
    "predictions = lrModel.transform(test_data)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6265734265734266"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='instr', metricName='accuracy')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5181398783178148"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='instr', metricName='f1')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECKING METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predicitons\n",
    "predictionAndTarget = lrModel.transform(test_data).select(\"instr\", \"prediction\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='instr')\n",
    "# Get metrics\n",
    "acc = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"weightedRecall\"})\n",
    "auc = evaluator.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6265734265734266\n",
      "0.5181398783178148\n",
      "0.5599402778030851\n",
      "0.6265734265734266\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(f1)\n",
    "print(weightedPrecision)\n",
    "print(weightedRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "\n",
    "predictionAndLabels = lrModel.transform(test_data).select('instr', 'prediction')\n",
    "metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
    "\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "labels = [int(l) for l in metrics.call('labels')]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix , index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>156.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1      2      3\n",
       "1    9.0    3.0    8.0\n",
       "2   10.0   22.0   20.0\n",
       "3  156.0  337.0  865.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETER TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\\\n",
    "             .addGrid(lr.regParam,[0.001,0.01,0.1,1])\\\n",
    "             .addGrid(lr.elasticNetParam,[0.0,0.5,1.0])\\\n",
    "             .build())\n",
    "\n",
    "# Create 4-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4993116485249862,\n",
       " {Param(parent='LogisticRegression_424d811739e639f5be2b', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "  Param(parent='LogisticRegression_424d811739e639f5be2b', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best Model Params\n",
    "score_params_list = list(zip(cvModel.avgMetrics, cvModel.getEstimatorParamMaps()))\n",
    "max(score_params_list,key=lambda item:item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5181332770507456"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cvModel.bestModel.transform(test_data)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predicitons\n",
    "predictionAndTarget = cvModel.transform(test_data).select(\"instr\", \"prediction\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='instr')\n",
    "# Get metrics\n",
    "acc = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluator.evaluate(predictionAndTarget, {evaluator.metricName: \"weightedRecall\"})\n",
    "auc = evaluator.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6265734265734266\n",
      "0.5181332770507456\n",
      "0.5608179292937531\n",
      "0.6265734265734266\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(f1)\n",
    "print(weightedPrecision)\n",
    "print(weightedRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predictionAndLabels = cvModel.transform(test_data).select('instr', 'prediction')\n",
    "metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
    "\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "labels = [int(l) for l in metrics.call('labels')]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix , index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>156.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1      2      3\n",
       "1    9.0    3.0    7.0\n",
       "2   10.0   22.0   21.0\n",
       "3  156.0  337.0  865.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION on SKLEARN gave ACCURACY SCORE 61% ,in PYSPARK RESULATED in SAME SCORE 62% EVEN AFTER TUNNING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCALED THE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(scaledFeatures=DenseVector([-1.4304, -0.3958, -1.1209, -1.3124, -1.4362, -1.6111, -1.736, -1.6195, -1.6454, -1.6391, -1.6099, -1.5935, -1.7075, -1.6382, -1.6961, -1.5663, -1.7696, -1.8233, -1.8179, -1.6803, -1.888, -1.7328, -1.7766, -1.7914, -1.8132, -1.8328, -1.7278, -1.6986, -1.8328, -1.7482, -1.671, -1.7975])),\n",
       " Row(scaledFeatures=DenseVector([-1.4304, -0.3958, -1.1209, -1.3124, -1.4362, -1.6111, -1.736, -1.6195, -1.6454, -1.6391, -1.6099, -1.5935, -1.7075, -1.6382, -1.6961, -1.5663, -1.7696, -1.8233, -1.8179, -1.6803, -1.888, -1.7328, -1.7766, -1.7914, -1.8132, -1.8328, -1.7278, -1.6986, -1.8328, -1.7482, -1.671, -1.7975])),\n",
       " Row(scaledFeatures=DenseVector([-1.4304, -0.3958, -1.1209, -1.3124, 0.0571, -0.0546, -1.736, -0.0602, -1.6454, -0.8577, -0.8276, -0.8115, -0.9191, -1.6382, -1.6961, -1.5663, -0.1853, 0.5727, 0.5763, -0.1251, -1.0978, 0.617, -1.7766, -0.2209, -0.2371, -0.2489, -0.9412, -0.1272, 0.5583, -0.9582, -0.119, -0.2338]))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(train_data)\n",
    "scaledData = scalerModel.transform(train_data)\n",
    "scaledData_test = scalerModel.transform(test_data)\n",
    "scaledData.select(\"scaledFeatures\").take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPLIED PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=5, inputCol=\"scaledFeatures\", outputCol=\"pca_features\")\n",
    "pcamodel = pca.fit(scaledData)\n",
    "pcatraindata=pcamodel.transform(scaledData)\n",
    "pca_test = pcamodel.transform(scaledData_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr1 = LogisticRegression(labelCol=\"instr\", featuresCol=\"scaledFeatures\",  \n",
    "                        maxIter=100, regParam=0.0001, family=\"multinomial\",  \n",
    "                        elasticNetParam=0.0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel1 = lr1.fit(pcatraindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- instr: double (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      " |-- nbrepeat: integer (nullable = true)\n",
      " |-- attendance: integer (nullable = true)\n",
      " |-- difficulty: integer (nullable = true)\n",
      " |-- Q1: integer (nullable = true)\n",
      " |-- Q2: integer (nullable = true)\n",
      " |-- Q3: integer (nullable = true)\n",
      " |-- Q4: integer (nullable = true)\n",
      " |-- Q5: integer (nullable = true)\n",
      " |-- Q6: integer (nullable = true)\n",
      " |-- Q7: integer (nullable = true)\n",
      " |-- Q8: integer (nullable = true)\n",
      " |-- Q9: integer (nullable = true)\n",
      " |-- Q10: integer (nullable = true)\n",
      " |-- Q11: integer (nullable = true)\n",
      " |-- Q12: integer (nullable = true)\n",
      " |-- Q13: integer (nullable = true)\n",
      " |-- Q14: integer (nullable = true)\n",
      " |-- Q15: integer (nullable = true)\n",
      " |-- Q16: integer (nullable = true)\n",
      " |-- Q17: integer (nullable = true)\n",
      " |-- Q18: integer (nullable = true)\n",
      " |-- Q19: integer (nullable = true)\n",
      " |-- Q20: integer (nullable = true)\n",
      " |-- Q21: integer (nullable = true)\n",
      " |-- Q22: integer (nullable = true)\n",
      " |-- Q23: integer (nullable = true)\n",
      " |-- Q24: integer (nullable = true)\n",
      " |-- Q25: integer (nullable = true)\n",
      " |-- Q26: integer (nullable = true)\n",
      " |-- Q27: integer (nullable = true)\n",
      " |-- Q28: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      " |-- pca_features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predpca = lrModel1.transform(pca_test)\n",
    "predpca.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget1 = lrModel1.transform(pca_test).select(\"instr\", \"prediction\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='instr')\n",
    "# Get metrics\n",
    "acc = evaluator.evaluate(predictionAndTarget1, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictionAndTarget1, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision = evaluator.evaluate(predictionAndTarget1, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluator.evaluate(predictionAndTarget1, {evaluator.metricName: \"weightedRecall\"})\n",
    "auc = evaluator.evaluate(predictionAndTarget1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.627972027972028\n",
      "0.5192429625673614\n",
      "0.5617376281515962\n",
      "0.627972027972028\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(f1)\n",
    "print(weightedPrecision)\n",
    "print(weightedRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabels1 = lrModel1.transform(pca_test).select('instr', 'prediction')\n",
    "metrics1 = MulticlassMetrics(predictionAndLabels1.rdd.map(lambda x: tuple(map(float, x))))\n",
    "\n",
    "confusion_matrix = metrics1.confusionMatrix().toArray()\n",
    "labels = [int(l) for l in metrics1.call('labels')]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix , index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>155.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1      2      3\n",
       "1    9.0    4.0    6.0\n",
       "2   11.0   22.0   20.0\n",
       "3  155.0  336.0  867.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE WITH PCA instr3 has one value increased in the TP ,one decrease each in FP FOR inst1 and isnt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION with PCA on SKLEARN gave ACCURACY SCORE 61% ,in PYSPARK with PCA RESULATED in SAME SCORE 62% EVEN AFTER TUNNING "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
